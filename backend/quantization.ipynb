{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\Python311\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\BarrettFrewA\\AppData\\Local\\Temp\\ipykernel_18444\\3560592278.py\", line 2, in <module>\n",
      "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\transformers\\__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\transformers\\utils\\__init__.py\", line 33, in <module>\n",
      "    from .generic import (\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py\", line 461, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\__init__.py\", line 2120, in <module>\n",
      "    from torch._higher_order_ops import cond\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\__init__.py\", line 1, in <module>\n",
      "    from .cond import cond\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\_higher_order_ops\\cond.py\", line 5, in <module>\n",
      "    import torch._subclasses.functional_tensor\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 42, in <module>\n",
      "    class FunctionalTensor(torch.Tensor):\n",
      "  File \"c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py\", line 258, in FunctionalTensor\n",
      "    cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n",
      "c:\\Users\\BarrettFrewA\\Jupyter Notebooks\\ABFLLMchatbot\\LLM\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if platform.system() == 'Darwin':  # macOS\n",
    "    torch.backends.quantized.engine = 'qnnpack'\n",
    "elif platform.system() == 'Windows':  # Windows\n",
    "    torch.backends.quantized.engine = 'fbgemm'\n",
    "else:\n",
    "    print(\"Unsupported platform for quantization engine\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying quantization...\n",
      "Applied quantization in 29.61 seconds\n",
      "Quantized model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Apply dynamic quantization\n",
    "start_time = time.time()\n",
    "print(\"Applying quantization...\")\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8  # Quantize the linear layers\n",
    ")\n",
    "print(f\"Applied quantization in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Save the quantized model's state dictionary\n",
    "torch.save(quantized_model.state_dict(), 'quantized_t5_large.pth')\n",
    "print(\"Quantized model saved successfully.\")\n",
    "\n",
    "# Define the tokenizer\n",
    "quantized_tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translation...\n",
      "Generated translation in 3.80 seconds\n",
      "Translation: Mon nom est Alana\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate translation\n",
    "input_text = \"translate English to French: My name is Alana\"\n",
    "input_ids = quantized_tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Generating translation...\")\n",
    "outputs = quantized_model.generate(input_ids, max_new_tokens=50)\n",
    "translation = quantized_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"Generated translation in {time.time() - start_time:.2f} seconds\")\n",
    "print(\"Translation:\", translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
